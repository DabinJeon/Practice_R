
Twitter Text-Mining

############################################################################################

install.packages(c("twitteR","httpuv", "tm", "wordcloud"))
install.packages("tm")
install.packages("wordcloud")
install.packages("httpuv")
install.packages("twitteR")

library(tm)
library(wordcloud)
library(httpuv)
library(twitteR)

setup_twitter_oauth(api_key,api_secret)

analyze <- function(lang, coin, date) {
    
  tweets = searchTwitter(enc2utf8(coin), n=500, since=date, lang=lang)
  texts = sapply(tweets, function(x) x$getText())
  corpus = Corpus(VectorSource(texts))
  corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = "byte"))
  
  # 실제로 문서에 어떤 단어가 출현하는지 나타내는 표를 term document matrix라고 합니다.
  tdm = TermDocumentMatrix(corpus,
                           control = list(removePunctuation = TRUE,
                                          stopwords = c("https",coin , stopwords("english")),
                                          removeNumbers = TRUE, tolower = TRUE))
  
  # 보기쉽게 매트릭스 (행렬) 형태로 변환
  m = as.matrix(tdm)
    
  # 빈도수 대로 내림차수 정렬
  word_freqs = sort(rowSums(m), decreasing=TRUE) 
  dm = data.frame(word=names(word_freqs), freq=word_freqs)
  
  print(word_freqs)
  print(dm)

  # 워드클라우드 생성
  wordcloud(words = dm$word, freq = dm$freq, min.freq = 5,
            max.words=200, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2"))
  
  return(dm)
  
}

dm = analyze("en","BCH", "2018-01-25")

############################################################################################

rm(list=ls())
library(twitteR)
library(ROAuth)
library("KoNLP")
library("rJava")
library("plyr")
library("RColorBrewer")
library("wordcloud")

consumer_key <- ""
consumer_secret <- ""
access_token <- ""
access_secret <- ""
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_secret)

# API 키를 로드
source("twitterOAuth.r")

# 데이터 추출
hp_tweets <- searchTwitter("@BTS",lang="ko",n=1000) #원문추출
hp_tweets.text <- strip_retweets(hp_tweets) # 리트윗 제거
hp_tweets_df <- twListToDF(hp_tweets.text) # 트윗 내용을 분석 가능한 데이터 프레임으로 변환# 관련 

tweets <- searchTwitter("@BTS",lang="ko",n=1000) 
no_retweets.text <- strip_retweets(_tweets) 
no_retweets_df <- twListToDF(no_retweets) 

head(no_retweets_df,20)

names(no_retweets_df)

tweet.text <- no_retweets_df$text
head(tweet.text)

tweet.text1 <- gsub("/n","",tweet.text)
tweet.text1 <- gsub("/r","",tweet.text1)
tweet.text1 <- gsub("http://","",tweet.text1)
tweet.text1 <- gsub("@","",tweet.text1)

head(tweet.text1)

useSejongDic()

tweet.noun <- Map(extractNoun, tweet.text1)
head(tweet.noun)

tweet.word <- unlist(tweet.noun, use.names=F)
head(tweet.word)

tweet.count <- table(tweet.word)
tweet.count1 <- tweet.count[tweet.count>10]
head(sort(tweet.count1,decreasing=T),20)

wordcloud(names(tweet.count1),freq=tweet.count1,scale=c(10,0.5),
min.freq=10,random.order=FALSE,color=brewer.pal(20,"Dark2"))

############################################################################################

